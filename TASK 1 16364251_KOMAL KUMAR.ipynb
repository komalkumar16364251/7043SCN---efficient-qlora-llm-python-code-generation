{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes rouge-score nltk"
      ],
      "metadata": {
        "id": "MxcWyCePEpZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmRPfohzCner"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"iamtarun/code_instructions_120k_alpaca\")\n",
        "\n",
        "print(\"Dataset loaded ✅\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_python(example):\n",
        "    return \"def \" in example[\"output\"] or \"import \" in example[\"output\"]\n",
        "\n",
        "filtered = dataset[\"train\"].filter(filter_python)\n",
        "\n",
        "print(\"Filtered size:\", len(filtered))"
      ],
      "metadata": {
        "id": "cvSEel6JElkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = filtered.shuffle(seed=42).select(range(4000))\n",
        "\n",
        "# Split into train / validation / test\n",
        "split = filtered.train_test_split(test_size=0.2, seed=42)\n",
        "test_valid = split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "train_dataset = split[\"train\"]\n",
        "val_dataset = test_valid[\"train\"]\n",
        "test_dataset = test_valid[\"test\"]\n",
        "\n",
        "print(\"Train:\", len(train_dataset))\n",
        "print(\"Validation:\", len(val_dataset))\n",
        "print(\"Test:\", len(test_dataset))"
      ],
      "metadata": {
        "id": "DpRtK7hmFCx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_example(example):\n",
        "    text = f\"\"\"Below is an instruction that describes a task.\n",
        "\n",
        "### Instruction:\n",
        "{example['instruction']}\n",
        "\n",
        "### Input:\n",
        "{example['input']}\n",
        "\n",
        "### Response:\n",
        "{example['output']}\"\"\"\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "train_dataset = train_dataset.map(format_example)\n",
        "val_dataset = val_dataset.map(format_example)\n",
        "\n",
        "print(\"Columns:\", train_dataset.column_names)"
      ],
      "metadata": {
        "id": "dgWBMY-aFIty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT TOKENIZER\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Tokenizer loaded successfully ✅\")"
      ],
      "metadata": {
        "id": "fsaRHrkoFaWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    tokens = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # Labels required for training\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "\n",
        "    return tokens\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Columns:\", train_dataset.column_names)"
      ],
      "metadata": {
        "id": "4dHgS7JzFlES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float32\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Set pad token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Model loaded with quantization ✅\")"
      ],
      "metadata": {
        "id": "yQkxWP3-GOB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Prepare model for QLoRA training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Define QLoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply adapters\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Show trainable params\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"QLoRA adapters applied successfully ✅\")"
      ],
      "metadata": {
        "id": "p1czfTY3GVKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qlora-output\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    max_steps=100,            # fast training for MSc project\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    save_total_limit=1,\n",
        "    fp16=False,               # safe setting\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "print(\"Training started...\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training completed successfully ✅\")"
      ],
      "metadata": {
        "id": "79MeqSzIGhzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"\"\"Below is an instruction.\n",
        "\n",
        "### Instruction:\n",
        "Write a Python function to add two numbers.\n",
        "\n",
        "### Input:\n",
        "\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "generated_code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Output:\\n\")\n",
        "print(generated_code)"
      ],
      "metadata": {
        "id": "Ybt18ZdhIDuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a, b):\n",
        "    return a + b"
      ],
      "metadata": {
        "id": "Y57L8JWVIsJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "bleu = load(\"bleu\")\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "prediction = generated_code\n",
        "\n",
        "reference = \"\"\"def add(a, b):\n",
        "    return a + b\"\"\"\n",
        "\n",
        "bleu_score = bleu.compute(\n",
        "    predictions=[prediction],\n",
        "    references=[[reference]]\n",
        ")\n",
        "\n",
        "rouge_score = rouge.compute(\n",
        "    predictions=[prediction],\n",
        "    references=[reference]\n",
        ")\n",
        "\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "print(\"ROUGE Score:\", rouge_score)"
      ],
      "metadata": {
        "id": "r_kgqLxBI6OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./qlora-trained-model\")\n",
        "tokenizer.save_pretrained(\"./qlora-trained-model\")\n",
        "\n",
        "print(\"QLoRA model saved successfully ✅\")"
      ],
      "metadata": {
        "id": "HPz7vsntJD9_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
