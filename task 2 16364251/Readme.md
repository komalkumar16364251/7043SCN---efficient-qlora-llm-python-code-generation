# Task 2 – Reinforcement Learning using PPO in ChefHat Environment

## Student Information

- Module: 7043SCN – Generative AI and Reinforcement Learning
- Task: Task 2 – Reinforcement Learning Implementation
- Student ID: 16364251
- Modulo Calculation: 16364251 mod 7 = 1
- Assigned Variant: Opponent Modelling Variant
- Algorithm Used: Proximal Policy Optimization (PPO)
- Environment: ChefHat Gym
- Framework: Stable-Baselines3
- Language: Python

---

## Assignment Variant – Opponent Modelling Variant

Based on my Student ID (16364251), the modulo 7 result is 1.

According to the assignment variant definitions:

ID mod 7 = 0 or 1 → Opponent Modelling Variant

In this project, the PPO reinforcement learning agent was trained in the ChefHat Gym environment where it interacts with opponent agents.

The agent learns optimal strategies by observing opponent actions, receiving rewards, and adapting its behaviour over time. This allows analysis of how the agent performs against opponent strategies and improves through experience.

This satisfies the Opponent Modelling Variant requirement.

---

## Project Overview

This project implements a Reinforcement Learning agent using the Proximal Policy Optimization (PPO) algorithm in the ChefHat Gym environment.

The objective is to train an intelligent agent that learns optimal strategies through interaction with the environment and improves its performance over time using reward-based learning.

The agent observes the environment, takes actions, receives rewards, and updates its policy to maximize long-term rewards.

---

## Tools and Libraries Used

The following tools and libraries were used:

- Python
- Jupyter Notebook
- Stable-Baselines3
- Gym / Gymnasium
- NumPy
- Matplotlib
- PyTorch
- Anaconda Environment

---

## Dataset

The dataset used in this project is:

Dataset.pkl.csv

This dataset contains simulated ChefHat game data generated from the ChefHat Gym environment. It is used for training and evaluation of the reinforcement learning agent.

---

## Methodology

The following steps were performed:

1. Installed and configured the ChefHat Gym environment.
2. Created the reinforcement learning environment.
3. Initialized the PPO agent using Stable-Baselines3.
4. Trained the agent through interaction with opponent agents.
5. Saved the trained model.
6. Generated performance graphs.
7. Evaluated the learning performance.

---

## PPO Algorithm

Proximal Policy Optimization (PPO) is a policy gradient method used in reinforcement learning.

Advantages of PPO:

- Stable training
- Efficient learning
- Good performance in complex environments
- Widely used in modern reinforcement learning applications

---

## Results

The following performance metrics were obtained:

- Learning Curve
- Average Reward Improvement
- Win Rate Analysis
- Episode Performance

The graphs show that the agent improves its performance over time through interaction with opponent agents.

---

## Outputs

The outputs folder contains:

- learning_curve.png
- avg_reward.png
- win_rate.png
- episode_length.png

These graphs demonstrate the learning progress of the PPO agent.

---

## Trained Model

The trained PPO model is saved as:

chefhat_ppo_model.zip

This file contains the learned policy and can be reused without retraining.

---

## How to Run the Project

Step 1: Install required libraries

Step 2: Open Jupyter Notebook

Step 3: Run all cells to train and evaluate the agent

---

## Conclusion

This project successfully implemented a PPO reinforcement learning agent in the ChefHat Gym environment.

The agent learned optimal strategies through interaction with opponent agents, and performance improved over time as shown in the output graphs.

This demonstrates the effectiveness of reinforcement learning and opponent modelling using PPO.

---

## Author

Student ID: 16364251  
Coventry University  
Module: 7043SCN – Generative AI and Reinforcement Learning
