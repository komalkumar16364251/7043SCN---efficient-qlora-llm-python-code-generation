# Efficient QLoRA Adaptation of a Compact LLM for Python Code Generation

## MSc Data Science and Computational Intelligence Project

This project implements efficient fine-tuning of a compact Large Language Model (LLM) for Python code generation using QLoRA (Quantized Low-Rank Adaptation). QLoRA enables parameter-efficient fine-tuning by freezing the base model weights and training low-rank adapter layers.

---

## Project Objectives

* Fine-tune a compact LLM using QLoRA
* Enable efficient Python code generation
* Reduce memory usage using quantization
* Evaluate model performance

---

## Technologies Used

* Python
* HuggingFace Transformers
* QLoRA
* PyTorch
* Google Colab

---

## Model Architecture

The project uses a Transformer-based Large Language Model with QLoRA adapters for efficient fine-tuning.

---

## Author

MSc Data Science and Computational Intelligence Student
